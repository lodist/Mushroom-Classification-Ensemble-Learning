{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARE DATA AND DEFINE MODEL\n",
    "\n",
    "# Define constants\n",
    "input_shape = (224, 224, 3)\n",
    "epochs = 30\n",
    "\n",
    "# Paths for folders with fewer images\n",
    "source_dir = 'path_to\\all_fungi'\n",
    "base_dir = 'path_to\\images'\n",
    "train_dir = 'path_to\\train'\n",
    "val_dir = 'path_to\\val'\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Function to count images in folders and move those with fewer images\n",
    "def move_folders_with_fewer_images(source_dir, target_dir, min_images=100):\n",
    "    for folder_name in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder_name)\n",
    "        target_folder_path = os.path.join(target_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            image_count = len([file for file in os.listdir(folder_path) if file.lower().endswith(('jpg', 'jpeg', 'png', 'bmp', 'tiff', 'webp'))])\n",
    "            if image_count > min_images:\n",
    "                if os.path.exists(target_folder_path):\n",
    "                    shutil.rmtree(target_folder_path)  # Remove existing directory if it exists\n",
    "                shutil.copytree(folder_path, target_folder_path)\n",
    "\n",
    "# Move folders with fewer images to the new directory\n",
    "move_folders_with_fewer_images(source_dir, base_dir)\n",
    "\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Rescale the image\n",
    "    return img_array\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(base_dir, train_dir, val_dir, split_ratio=0.8):\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.makedirs(val_dir)\n",
    "\n",
    "    supported_extensions = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp']\n",
    "\n",
    "    for subdir, dirs, files in os.walk(base_dir):\n",
    "        if subdir == base_dir:\n",
    "            continue\n",
    "        image_files = [f for f in files if any(f.lower().endswith(ext) for ext in supported_extensions)]\n",
    "\n",
    "        if not image_files:\n",
    "            continue\n",
    "\n",
    "        train_files, val_files = train_test_split(image_files, train_size=split_ratio, random_state=42)\n",
    "        class_name = os.path.basename(subdir)\n",
    "        train_class_dir = os.path.join(train_dir, class_name)\n",
    "        val_class_dir = os.path.join(val_dir, class_name)\n",
    "\n",
    "        if not os.path.exists(train_class_dir):\n",
    "            os.makedirs(train_class_dir)\n",
    "        if not os.path.exists(val_class_dir):\n",
    "            os.makedirs(val_class_dir)\n",
    "\n",
    "        for file_name in train_files:\n",
    "            shutil.copy(os.path.join(subdir, file_name), os.path.join(train_class_dir, file_name))\n",
    "\n",
    "        for file_name in val_files:\n",
    "            shutil.copy(os.path.join(subdir, file_name), os.path.join(val_class_dir, file_name))\n",
    "\n",
    "    print(\"Data preparation complete.\")\n",
    "\n",
    "# Function to get data loaders for a subset of classes\n",
    "def get_data_loaders_subset(train_dir, val_dir, class_subset):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  # Add vertical flipping\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    train_loader = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='sparse',\n",
    "        classes=class_subset\n",
    "    )\n",
    "\n",
    "    val_loader = ImageDataGenerator(\n",
    "        rescale=1.0/255\n",
    "    ).flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='sparse',\n",
    "        classes=class_subset\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    # Unfreeze some layers of the base model for fine-tuning\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[100:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Adjust learning rate for fine-tuning\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, model_path, epochs=30):\n",
    "    \"\"\"Train the model with the given training and validation data.\"\"\"\n",
    "    # Define callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path,  # Save the best model based on validation loss\n",
    "        monitor='val_loss',  # Monitor the validation loss\n",
    "        save_best_only=True,  # Save only the best model\n",
    "        verbose=1  # Print messages when saving the model\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',  # Monitor the validation accuracy\n",
    "        patience=20,  # Increase patience\n",
    "        verbose=1,  # Print messages when stopping the training\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    "    )\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_loader,  # Training data\n",
    "        epochs=epochs,  # Number of epochs to train\n",
    "        validation_data=val_loader,  # Validation data\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr]  # Callbacks to use during training\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Prepare and organize data\n",
    "prepare_data(base_dir, train_dir, val_dir)\n",
    "\n",
    "# Get the class names from the directory names, assuming they are sorted alphabetically\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "\n",
    "# Split the class names into four subsets\n",
    "split_size = len(class_names) // 4\n",
    "class_names_split = [class_names[i:i + split_size] for i in range(0, len(class_names), split_size)]\n",
    "\n",
    "# Ensure the last group includes any remaining classes\n",
    "if len(class_names_split) > 4:\n",
    "    class_names_split[-2].extend(class_names_split[-1])\n",
    "    class_names_split = class_names_split[:-1]\n",
    "\n",
    "# Print the splits\n",
    "for i, split in enumerate(class_names_split):\n",
    "    print(f\"Group {i + 1}: {len(split)} classes\")\n",
    "    print(split)\n",
    "\n",
    "# Save the class names split\n",
    "with open('class_names_split.pickle', 'wb') as handle:\n",
    "    pickle.dump(class_names_split, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Class names split saved.\")\n",
    "\n",
    "model_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#TRAIN THE SEPARATE MODELS\n",
    "\n",
    "# Loop and train models for each class subset\n",
    "for i, class_subset in enumerate(class_names_split):\n",
    "    print(f\"Training model for class subset {i + 1}\")\n",
    "    train_loader, val_loader = get_data_loaders_subset(train_dir, val_dir, class_subset)\n",
    "    model = build_model(input_shape, len(class_subset))\n",
    "    model_path = f'mushroom_classification_model_{i}.keras'\n",
    "    history = train_model(model, train_loader, val_loader, model_path, epochs)\n",
    "    model.save(model_path)\n",
    "    model_paths.append(model_path)\n",
    "\n",
    "print(\"Model training complete. Model paths:\", model_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT MODELS TO TFLITE\n",
    "\n",
    "# List of model paths\n",
    "model_paths = [\n",
    "    'mushroom_classification_model_0.keras',\n",
    "    'mushroom_classification_model_1.keras',\n",
    "    'mushroom_classification_model_2.keras',\n",
    "    'mushroom_classification_model_3.keras'\n",
    "]\n",
    "\n",
    "# Function to convert a model to TensorFlow Lite\n",
    "def convert_to_tflite(model_path):\n",
    "    # Load the Keras model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Convert the model to TensorFlow Lite format\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save the converted model\n",
    "    tflite_model_path = model_path.replace('.keras', '.tflite')\n",
    "    with open(tflite_model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Converted {model_path} to {tflite_model_path}\")\n",
    "\n",
    "# Convert all models\n",
    "for model_path in model_paths:\n",
    "    convert_to_tflite(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#APPLY MODELS\n",
    "\n",
    "# Initialize list to store model paths\n",
    "model_paths = []\n",
    "\n",
    "# Generate model paths\n",
    "for i in range(4):  # Assuming there are 4 models\n",
    "    model_path = f'mushroom_classification_model_{i}.tflite'\n",
    "    model_paths.append(model_path)\n",
    "\n",
    "print(\"Model paths:\", model_paths)\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Rescale the image\n",
    "    return img_array\n",
    "\n",
    "def load_tflite_model(model_path):\n",
    "    # Load the TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    return interpreter, input_details, output_details\n",
    "\n",
    "def predict_with_tflite(interpreter, input_details, output_details, img_array):\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return predictions\n",
    "\n",
    "def clean_class_name(class_name):\n",
    "    # Remove numbers and underscores, and replace with spaces\n",
    "    cleaned_name = re.sub(r'^\\d+_', '', class_name).replace('_', ' ')\n",
    "    # Capitalize each word\n",
    "    cleaned_name = ' '.join(word.capitalize() for word in cleaned_name.split())\n",
    "    return cleaned_name\n",
    "\n",
    "def predict_ensemble(image_path, model_paths, class_subset_path):\n",
    "    img_array = preprocess_image(image_path)\n",
    "    all_predictions = []\n",
    "\n",
    "    # Load the class subsets once\n",
    "    with open(class_subset_path, 'rb') as handle:\n",
    "        class_subsets = pickle.load(handle)\n",
    "\n",
    "    # Load each model and use corresponding class subset\n",
    "    for model_path, class_subset in zip(model_paths, class_subsets):\n",
    "        interpreter, input_details, output_details = load_tflite_model(model_path)\n",
    "        predictions = predict_with_tflite(interpreter, input_details, output_details, img_array)\n",
    "        all_predictions.append((predictions, class_subset))\n",
    "\n",
    "    # Combine predictions from all models\n",
    "    combined_predictions = {}\n",
    "    for predictions, class_subset in all_predictions:\n",
    "        for idx, class_name in enumerate(class_subset):\n",
    "            if class_name in combined_predictions:\n",
    "                combined_predictions[class_name] += predictions[0][idx]\n",
    "            else:\n",
    "                combined_predictions[class_name] = predictions[0][idx]\n",
    "\n",
    "    # Normalize and sort the predictions\n",
    "    total = sum(combined_predictions.values())\n",
    "    for key in combined_predictions:\n",
    "        combined_predictions[key] /= total\n",
    "\n",
    "    sorted_predictions = sorted(combined_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_3_predictions = sorted_predictions[:3]\n",
    "\n",
    "    # Clean class names\n",
    "    cleaned_predictions = [(clean_class_name(class_name), prob) for class_name, prob in top_3_predictions]\n",
    "\n",
    "    return cleaned_predictions\n",
    "\n",
    "def get_top_prediction(predictions):\n",
    "    return predictions[0] if predictions else None\n",
    "\n",
    "# Usage\n",
    "model_paths = [f'mushroom_classification_model_{i}.tflite' for i in range(4)]\n",
    "class_subset_path = 'class_names_split.pickle'  # This is the correct path to the pickle file\n",
    "image_path = 'path_to/image.jpg'\n",
    "\n",
    "top_3_predictions = predict_ensemble(image_path, model_paths, class_subset_path)\n",
    "\n",
    "# Print top 3 predictions\n",
    "print(\"Top 3 Predictions:\")\n",
    "for predicted_class, predicted_probability in top_3_predictions:\n",
    "    print(f'Predicted class: {predicted_class}, Probability: {predicted_probability:.2f}')\n",
    "\n",
    "# Get and print the top prediction\n",
    "top_prediction = get_top_prediction(top_3_predictions)\n",
    "if top_prediction:\n",
    "    predicted_class, predicted_probability = top_prediction\n",
    "    print(f'\\nTop Prediction:\\nPredicted class: {predicted_class}, Probability: {predicted_probability:.2f}')\n",
    "else:\n",
    "    print(\"No predictions available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
